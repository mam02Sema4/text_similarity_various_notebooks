{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"8-ImplementingBERTSimilarity.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPx+4YpcC0jDeU1cWRJaejN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"a_SaV8-aymqf"},"source":["#A lower-level implementation of a sentence embedding with PyTorch and transformers.\n","\n","In this notebook, we will develop our transformation to the last_hidden_state to create the sentence embedding. For this, we perform the mean pooling operation."]},{"cell_type":"code","metadata":{"id":"W0E9xMNNUs13","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626085053924,"user_tz":-120,"elapsed":3414,"user":{"displayName":"ISABEL SEGURA BEDMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQ1FNrqSnUE2MEN5-mH-AjUf5Ch9orkmSxxfel=s64","userId":"10362143810849156637"}},"outputId":"b9e293fc-dbb9-43db-e3be-2ddae4a7a939"},"source":["!pip install -U transformers\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.0.0)\n","Requirement already satisfied, skipping upgrade: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.0.13)\n","Requirement already satisfied, skipping upgrade: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.8.2)\n","Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n","Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n","Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n","Requirement already satisfied, skipping upgrade: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.0+cu102)\n","Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n","Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.9.0+cu102)\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n","Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n","Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->sentence-transformers) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->sentence-transformers) (2.23.0)\n","Requirement already satisfied, skipping upgrade: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->sentence-transformers) (20.9)\n","Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->sentence-transformers) (4.6.0)\n","Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->sentence-transformers) (3.0.12)\n","Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.13)\n","Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.45)\n","Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.10.3)\n","Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n","Requirement already satisfied, skipping upgrade: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n","Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->sentence-transformers) (2.10)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->sentence-transformers) (3.0.4)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->sentence-transformers) (2021.5.30)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->sentence-transformers) (1.24.3)\n","Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub->sentence-transformers) (2.4.7)\n","Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->huggingface-hub->sentence-transformers) (3.4.1)\n","Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Nkor6xDKy4e_"},"source":["from transformers import AutoTokenizer, AutoModel\n","import torch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Db_GzVSvzARL"},"source":["First we initialize our model and tokenizer:\n"]},{"cell_type":"code","metadata":{"id":"Bua_DGCIy63D"},"source":["tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-MiniLM-L6-v2')\n","model = AutoModel.from_pretrained('sentence-transformers/paraphrase-MiniLM-L6-v2')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"scpdi-wFzGq7"},"source":[""]},{"cell_type":"code","metadata":{"id":"mPucbRtHzJBl"},"source":["sentences = [\n","    \"Three years later, the coffin was still full of Jello.\",\n","    \"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.\",\n","    \"The person box was packed with jelly many dozens of months later.\",\n","    \"He found a leprechaun in his walnut shell.\"\n","]\n","\n","# initialize dictionary to store tokenized sentences\n","tokens = {'input_ids': [], 'attention_mask': []}\n","\n","for sentence in sentences:\n","    # encode each sentence and append to dictionary\n","    #Tokenize and prepare for the model a sequence or a pair of sequences.\n","    new_tokens = tokenizer.encode_plus(sentence, max_length=128,\n","                                       truncation=True, padding='max_length',\n","                                       return_tensors='pt') #Return PyTorch torch.Tensor objects.\n","\n","   #The method encode_plus returns the List of token ids to be fed to a model, List of indices specifying which tokens should be attended to by the model \n","\n","    tokens['input_ids'].append(new_tokens['input_ids'][0])\n","    tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n","\n","# reformat list of tensors into single tensor. Concatenates a sequence of tensors along a new dimension.\n","tokens['input_ids'] = torch.stack(tokens['input_ids'])\n","tokens['attention_mask'] = torch.stack(tokens['attention_mask'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pybdt-4p0O4N","executionInfo":{"status":"ok","timestamp":1626080095346,"user_tz":-120,"elapsed":563,"user":{"displayName":"ISABEL SEGURA BEDMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQ1FNrqSnUE2MEN5-mH-AjUf5Ch9orkmSxxfel=s64","userId":"10362143810849156637"}},"outputId":"ee4b9957-b94d-4f29-8639-42c51a2a6a25"},"source":["outputs = model(**tokens)\n","outputs.keys()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["odict_keys(['last_hidden_state', 'pooler_output'])"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"0kzXY-4T0UbG"},"source":["The dense vector representations of our text are contained within the outputs 'last_hidden_state' tensor, which we access like so:\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FQKiUrrA0TUy","executionInfo":{"status":"ok","timestamp":1626080098557,"user_tz":-120,"elapsed":167,"user":{"displayName":"ISABEL SEGURA BEDMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQ1FNrqSnUE2MEN5-mH-AjUf5Ch9orkmSxxfel=s64","userId":"10362143810849156637"}},"outputId":"b4901a35-a0b2-4f28-b767-07a74e707174"},"source":["embeddings = outputs.last_hidden_state\n","embeddings\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 9.0630e-02,  4.5589e-01, -1.5872e-01,  ..., -3.1558e-01,\n","           5.8567e-02, -1.7566e-01],\n","         [-2.1225e-01, -5.7729e-01, -2.0000e-01,  ..., -1.4099e-01,\n","           1.5964e-01,  7.6760e-01],\n","         [-2.1763e-01,  1.1818e+00, -2.7912e-01,  ..., -4.7069e-01,\n","           1.8733e-01,  1.8711e-01],\n","         ...,\n","         [-1.2064e-01,  6.1632e-02,  1.4978e-01,  ...,  2.3516e-01,\n","          -2.2291e-02, -1.6768e-01],\n","         [-1.4511e-01,  9.7930e-02,  1.5787e-01,  ...,  2.1651e-01,\n","           6.4461e-02, -1.2910e-01],\n","         [-1.2381e-01,  1.5829e-01,  1.6246e-01,  ...,  2.2581e-01,\n","           1.3747e-01, -8.8168e-02]],\n","\n","        [[ 1.3628e-01,  4.8279e-01,  2.6579e-01,  ...,  5.7575e-01,\n","          -1.1533e-01, -9.7986e-02],\n","         [-5.6376e-02,  1.2054e+00,  3.1291e-01,  ...,  1.1160e+00,\n","           6.6876e-01,  1.0406e+00],\n","         [ 5.0828e-01,  5.0805e-01,  1.8537e-01,  ...,  8.1560e-01,\n","           1.2319e+00,  3.0367e-02],\n","         ...,\n","         [-6.3757e-02,  1.6254e-01,  2.5266e-01,  ...,  4.7171e-01,\n","          -2.2942e-01, -2.5515e-01],\n","         [-8.1587e-02,  1.6165e-01,  2.2880e-01,  ...,  4.4723e-01,\n","          -1.4898e-01, -2.3954e-01],\n","         [-6.5276e-02,  2.0718e-01,  2.3461e-01,  ...,  4.5928e-01,\n","          -1.0623e-01, -1.8732e-01]],\n","\n","        [[ 1.8531e-01,  1.3400e-01, -3.9048e-01,  ..., -4.6091e-01,\n","          -4.1342e-02, -1.2918e-01],\n","         [ 2.2098e-01,  4.0388e-01, -7.3550e-01,  ..., -3.4227e-01,\n","           4.7195e-01,  5.2234e-01],\n","         [-6.0668e-01,  3.5911e-01, -2.7252e-01,  ..., -3.0267e-01,\n","           1.6224e-02,  2.6516e-01],\n","         ...,\n","         [-8.8223e-02, -5.7091e-02, -6.8084e-03,  ...,  3.7588e-02,\n","          -1.0479e-01, -2.5480e-01],\n","         [-7.2962e-02, -4.9242e-02, -1.0527e-02,  ...,  1.0540e-02,\n","           5.9924e-04, -2.2227e-01],\n","         [-3.4588e-02,  2.0457e-02, -7.5629e-03,  ...,  2.0983e-02,\n","           6.3794e-02, -1.8892e-01]],\n","\n","        [[ 2.5604e-01, -6.8386e-02, -1.9207e-01,  ...,  2.2870e-01,\n","          -1.8055e-01,  2.1359e-01],\n","         [-6.9140e-02,  9.6756e-01, -2.6087e-01,  ..., -2.8444e-01,\n","           3.4754e-01, -1.0195e-01],\n","         [-4.5793e-01,  3.9941e-01,  2.4248e-01,  ...,  1.7530e-01,\n","           4.7382e-01,  4.5644e-01],\n","         ...,\n","         [-2.1807e-01,  1.4103e-01,  1.7313e-01,  ...,  3.2634e-01,\n","          -7.6001e-02, -1.1684e-01],\n","         [-2.1780e-01,  1.5282e-01,  1.4877e-01,  ...,  3.0910e-01,\n","           2.0943e-02, -9.2314e-02],\n","         [-1.8449e-01,  2.1599e-01,  1.5184e-01,  ...,  3.1683e-01,\n","           7.2124e-02, -4.7853e-02]]], grad_fn=<NativeLayerNormBackward>)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"rBnp4EAm0fd8"},"source":["After we have produced our dense vectors embeddings, we need to perform a mean pooling operation to create a single vector encoding (the sentence embedding).\n","To do this mean pooling operation, we will need to multiply each value in our embeddings tensor by its respective attention_mask value — so that we ignore non-real tokens."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kbqplaok0peV","executionInfo":{"status":"ok","timestamp":1626080102476,"user_tz":-120,"elapsed":166,"user":{"displayName":"ISABEL SEGURA BEDMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQ1FNrqSnUE2MEN5-mH-AjUf5Ch9orkmSxxfel=s64","userId":"10362143810849156637"}},"outputId":"10936b0f-ab28-47e7-868b-1224dc589d5b"},"source":["attention_mask = tokens['attention_mask']\n","attention_mask.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 128])"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"HOW74xhg2BW_"},"source":["To perform this operation, we first resize our attention_mask tensor:\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QFfmBByy0sPA","executionInfo":{"status":"ok","timestamp":1626080104771,"user_tz":-120,"elapsed":202,"user":{"displayName":"ISABEL SEGURA BEDMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQ1FNrqSnUE2MEN5-mH-AjUf5Ch9orkmSxxfel=s64","userId":"10362143810849156637"}},"outputId":"8cfdf5df-15a3-4089-c0bd-b39878b73fa0"},"source":["\n","mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n","mask.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 128, 384])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2A7qdJbV0zEL","executionInfo":{"status":"ok","timestamp":1626080107197,"user_tz":-120,"elapsed":179,"user":{"displayName":"ISABEL SEGURA BEDMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQ1FNrqSnUE2MEN5-mH-AjUf5Ch9orkmSxxfel=s64","userId":"10362143810849156637"}},"outputId":"40920f72-9da9-469b-e9a4-c1066b034bb0"},"source":["mask"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n","         [1., 1., 1.,  ..., 1., 1., 1.],\n","         [1., 1., 1.,  ..., 1., 1., 1.],\n","         ...,\n","         [0., 0., 0.,  ..., 0., 0., 0.],\n","         [0., 0., 0.,  ..., 0., 0., 0.],\n","         [0., 0., 0.,  ..., 0., 0., 0.]],\n","\n","        [[1., 1., 1.,  ..., 1., 1., 1.],\n","         [1., 1., 1.,  ..., 1., 1., 1.],\n","         [1., 1., 1.,  ..., 1., 1., 1.],\n","         ...,\n","         [0., 0., 0.,  ..., 0., 0., 0.],\n","         [0., 0., 0.,  ..., 0., 0., 0.],\n","         [0., 0., 0.,  ..., 0., 0., 0.]],\n","\n","        [[1., 1., 1.,  ..., 1., 1., 1.],\n","         [1., 1., 1.,  ..., 1., 1., 1.],\n","         [1., 1., 1.,  ..., 1., 1., 1.],\n","         ...,\n","         [0., 0., 0.,  ..., 0., 0., 0.],\n","         [0., 0., 0.,  ..., 0., 0., 0.],\n","         [0., 0., 0.,  ..., 0., 0., 0.]],\n","\n","        [[1., 1., 1.,  ..., 1., 1., 1.],\n","         [1., 1., 1.,  ..., 1., 1., 1.],\n","         [1., 1., 1.,  ..., 1., 1., 1.],\n","         ...,\n","         [0., 0., 0.,  ..., 0., 0., 0.],\n","         [0., 0., 0.,  ..., 0., 0., 0.],\n","         [0., 0., 0.,  ..., 0., 0., 0.]]])"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"wPT9b2_32GPX"},"source":["Each vector above represents a single token attention mask - each token now has a vector of size 768 representing it's attention_mask status. Then we multiply the two tensors to apply the attention mask:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XgplGR3C1A-j","executionInfo":{"status":"ok","timestamp":1626080110078,"user_tz":-120,"elapsed":162,"user":{"displayName":"ISABEL SEGURA BEDMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQ1FNrqSnUE2MEN5-mH-AjUf5Ch9orkmSxxfel=s64","userId":"10362143810849156637"}},"outputId":"1299a44a-861c-4520-df57-b23b5784a017"},"source":["masked_embeddings = embeddings * mask\n","masked_embeddings.shape\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 128, 384])"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kjV-qST31FR0","executionInfo":{"status":"ok","timestamp":1626080111987,"user_tz":-120,"elapsed":198,"user":{"displayName":"ISABEL SEGURA BEDMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQ1FNrqSnUE2MEN5-mH-AjUf5Ch9orkmSxxfel=s64","userId":"10362143810849156637"}},"outputId":"ea2acb7d-a6d6-453b-e294-b044f467443c"},"source":["masked_embeddings\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 0.0906,  0.4559, -0.1587,  ..., -0.3156,  0.0586, -0.1757],\n","         [-0.2122, -0.5773, -0.2000,  ..., -0.1410,  0.1596,  0.7676],\n","         [-0.2176,  1.1818, -0.2791,  ..., -0.4707,  0.1873,  0.1871],\n","         ...,\n","         [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n","         [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n","         [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000]],\n","\n","        [[ 0.1363,  0.4828,  0.2658,  ...,  0.5758, -0.1153, -0.0980],\n","         [-0.0564,  1.2054,  0.3129,  ...,  1.1160,  0.6688,  1.0406],\n","         [ 0.5083,  0.5080,  0.1854,  ...,  0.8156,  1.2319,  0.0304],\n","         ...,\n","         [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n","         [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n","         [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000]],\n","\n","        [[ 0.1853,  0.1340, -0.3905,  ..., -0.4609, -0.0413, -0.1292],\n","         [ 0.2210,  0.4039, -0.7355,  ..., -0.3423,  0.4719,  0.5223],\n","         [-0.6067,  0.3591, -0.2725,  ..., -0.3027,  0.0162,  0.2652],\n","         ...,\n","         [-0.0000, -0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000],\n","         [-0.0000, -0.0000, -0.0000,  ...,  0.0000,  0.0000, -0.0000],\n","         [-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000, -0.0000]],\n","\n","        [[ 0.2560, -0.0684, -0.1921,  ...,  0.2287, -0.1805,  0.2136],\n","         [-0.0691,  0.9676, -0.2609,  ..., -0.2844,  0.3475, -0.1019],\n","         [-0.4579,  0.3994,  0.2425,  ...,  0.1753,  0.4738,  0.4564],\n","         ...,\n","         [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n","         [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n","         [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000]]],\n","       grad_fn=<MulBackward0>)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"n18C3c1_2OF8"},"source":["Then we sum the remained of the embeddings along axis 1:\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ln5PBINt1KFd","executionInfo":{"status":"ok","timestamp":1626080115388,"user_tz":-120,"elapsed":162,"user":{"displayName":"ISABEL SEGURA BEDMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQ1FNrqSnUE2MEN5-mH-AjUf5Ch9orkmSxxfel=s64","userId":"10362143810849156637"}},"outputId":"af491ad5-ab7a-443d-ed80-4757100f5dc8"},"source":["summed = torch.sum(masked_embeddings, 1)\n","summed.shape\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 384])"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"QQvntCiF2RBV"},"source":["Then sum the number of values that must be given attention in each position of the tensor:\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DC8nB9ub1Nzp","executionInfo":{"status":"ok","timestamp":1626080117416,"user_tz":-120,"elapsed":163,"user":{"displayName":"ISABEL SEGURA BEDMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQ1FNrqSnUE2MEN5-mH-AjUf5Ch9orkmSxxfel=s64","userId":"10362143810849156637"}},"outputId":"7d0f64d9-ef4f-442b-cc14-cfb4d1328feb"},"source":["summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n","summed_mask.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 384])"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"VDG1CsX_2VzX"},"source":["Finally, we calculate the mean as the sum of the embedding activations summed divided by the number of values that should be given attention in each position summed_mask:\n","\n"]},{"cell_type":"code","metadata":{"id":"5CwiIhlp1Qmq"},"source":["mean_pooled = summed / summed_mask\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tEsGmDEM2ZZ1"},"source":["Once we have our dense vectors, we can calculate the cosine similarity between each — which is the same logic we used before:\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bR6E2hr-03rg","executionInfo":{"status":"ok","timestamp":1626080122768,"user_tz":-120,"elapsed":570,"user":{"displayName":"ISABEL SEGURA BEDMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQ1FNrqSnUE2MEN5-mH-AjUf5Ch9orkmSxxfel=s64","userId":"10362143810849156637"}},"outputId":"c126bb5d-33e9-40d5-caa3-ba5a91fd55b2"},"source":["from sklearn.metrics.pairwise import cosine_similarity\n","# convert from PyTorch tensor to numpy array\n","mean_pooled = mean_pooled.detach().numpy()\n","\n","# calculate\n","cosine_similarity(\n","    [mean_pooled[0]],\n","    mean_pooled[1:]\n",")\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.13260713, 0.31947678, 0.17322943]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"CpdwLMh97bE6"},"source":["Comparamos con los resultados que obtiene el modelo directamente:\n"]},{"cell_type":"code","metadata":{"id":"9CmIg0JF7k1p"},"source":["!pip install -U sentence-transformers\n","\n","from sentence_transformers import SentenceTransformer\n","model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"03YcTDBn7oPn","executionInfo":{"status":"ok","timestamp":1626080225092,"user_tz":-120,"elapsed":201,"user":{"displayName":"ISABEL SEGURA BEDMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQ1FNrqSnUE2MEN5-mH-AjUf5Ch9orkmSxxfel=s64","userId":"10362143810849156637"}},"outputId":"880ed70a-6594-45ff-8073-f51d33e838c8"},"source":["sentences = [\n","    \"Three years later, the coffin was still full of Jello.\",\n","    \"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.\",\n","    \"The person box was packed with jelly many dozens of months later.\",\n","    \"He found a leprechaun in his walnut shell.\"\n","]\n","\n","sentence_embeddings = model.encode(sentences)\n","cosine_similarity(\n","    [sentence_embeddings[0]],\n","    sentence_embeddings[1:]\n",")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.13260716, 0.31947678, 0.17322943]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"4w3MDnOQFO2p"},"source":["\n","Seguir por aquí:\n","https://towardsdatascience.com/bert-for-measuring-text-similarity-eec91c6bf9e1"]}]}