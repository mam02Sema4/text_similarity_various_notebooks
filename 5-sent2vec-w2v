{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5-sent2vec-w2v","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPn6aSID8zr7UGKCiN2XAS1"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cjML9dWlYuv5"},"source":["# Text similarity by sent2vec\n","\n","Encoding is the task of representing text data (words, sentences or documents) into high-dimensional vectors. \n","\n","Sentence embedding (representation of sentences by vectors) helps us to tackle NLP tasks such as text classification or text summarization. \n","\n","In the past, we mostly used encondes such as  one-hot, term-frequency, or TF-IDF. The main drawback of these methods is that they are not able to capture syntactic and semantic information. \n","\n","The recent advancements in deep larning (such as neural word embedding models or BERT) are able to encode sentences or words in more meaningful forms. \n","\n","In this notebook, we learn how to work with the open-source **sent2vec** Python library to encode sentences. We will see two different methods to calculate the text similarity between two texts:\n","- Using word2vec\n","- By using BERT\n","\n","\n","Source: https://towardsdatascience.com/how-to-compute-sentence-similarity-using-bert-and-word2vec-ab0663a5d64\n"]},{"cell_type":"markdown","metadata":{"id":"ZcL57X9XaBEN"},"source":["First of all, we need to install this library.\n","\n","Note: sent2vec has dependencies with other libraries such as Spacy (for text cleaning), Gensim (for word2vec models), and Transformers (for various forms of BERT model). Maybe, you also need to install them. "]},{"cell_type":"code","metadata":{"id":"1fa1okj5Ytb2"},"source":["!pip3 install sent2vec\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nz4pYg0SDFMt"},"source":["## 1) Using Word2Vec  \n","\n","We can also use a word2vec approach to encode the sentences. \n","If you want to use a word2vec approach instead, you must first split sentences into lists of words using the sent2words method from the Splitter class in this library. This methods allows us to customize the list of stopwords. That is, you can add new stop words or remove some stop-words from the default stop-word list. For example, we can remove the stop-word 'not' from the list.\n","\n","\n","When you extract the most important words in sentences, you can compute the sentence embeddings using the word2vec method from the Vectorizer class. This method computes the average of vectors corresponding to the remaining (i.e., the most important) words using the code below."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F-PHmlTzDFr5","executionInfo":{"status":"ok","timestamp":1626084225771,"user_tz":-120,"elapsed":288,"user":{"displayName":"ISABEL SEGURA BEDMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQ1FNrqSnUE2MEN5-mH-AjUf5Ch9orkmSxxfel=s64","userId":"10362143810849156637"}},"outputId":"b2f1434e-1524-4c5d-960d-97ae961809ab"},"source":["from sent2vec.splitter import Splitter\n","import string \n","\n","sentences = [\n","    \"Boris Johnson congratulates Joe Biden on US election win.\",\n","    \"Boris Johnson congratulates Joe Biden on 2020 election.\",\n","    \"Putin does not congratulate Joe Biden on US election win.\",\n","    \"George W. Bush congratulates Joe Biden on 2020 election.\"\n","]\n","\n","#we create an Vectorizer object to represent the sentences\n","splitter = Splitter()\n","#method to split a sentence into tokens. You can add new stop words o remove stop words from\n","#the default stop word list.\n","splitter.sent2words(sentences=sentences, remove_stop_words=['not'], add_stop_words=[])\n","words=splitter.words\n","tokens=[]\n","for w in words:\n","    if w not in enumerate(string.punctuation):\n","        tokens.append(w)\n","\n","\n","print(tokens)\n","# [['joe', 'biden', 'president', 'united', 'states'], ['joe', 'biden', 'not', 'president', 'united', 'states']]"],"execution_count":15,"outputs":[{"output_type":"stream","text":["[['boris', 'johnson', 'congratulates', 'joe', 'biden', 'election', 'win'], ['boris', 'johnson', 'congratulates', 'joe', 'biden', '2020', 'election'], ['putin', 'not', 'congratulate', 'joe', 'biden', 'election', 'win'], ['george', 'w.', 'bush', 'congratulate', 'joe', 'biden', '2020', 'election']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l-fuctOChoWf"},"source":["Once sentences are tokenized, you can use the method **word2vec**,  which computes the average of vectors corresponding to the most important words:"]},{"cell_type":"code","metadata":{"id":"KAGyfYJAj-OG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626084229286,"user_tz":-120,"elapsed":234,"user":{"displayName":"ISABEL SEGURA BEDMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQ1FNrqSnUE2MEN5-mH-AjUf5Ch9orkmSxxfel=s64","userId":"10362143810849156637"}},"outputId":"12f97c15-c1a5-4c4c-d482-335e716f54de"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","root='drive/My Drive/Colab Notebooks/resourcesNLP/'\n"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tkyAVI6vhzmI","colab":{"base_uri":"https://localhost:8080/","height":371},"executionInfo":{"status":"error","timestamp":1626084514883,"user_tz":-120,"elapsed":9747,"user":{"displayName":"ISABEL SEGURA BEDMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQ1FNrqSnUE2MEN5-mH-AjUf5Ch9orkmSxxfel=s64","userId":"10362143810849156637"}},"outputId":"d229aa68-9b67-4be9-f5a0-5d1344e8e7a5"},"source":["from sent2vec.vectorizer import Vectorizer\n","\n","#PRETRAINED_VECTORS_PATH=root+'glove.840B.300d.txt'\n","PRETRAINED_VECTORS_PATH=root+'GoogleNews-vectors-negative300.bin'\n","\n","\n","#word_vectors = KeyedVectors.load('model.bin')\n","#print(\"vector for mother:\", word_vectors['mother'])\n","\n","vectorizer = Vectorizer()\n","vectorizer.word2vec(tokens, pretrained_vectors_path=PRETRAINED_VECTORS_PATH)\n","vectors_w2v = vectorizer.vectors\n","\n","#dist_w2v = spatial.distance.cosine(vectors_w2v[0], vectors_w2v[1])\n","#print('dist_w2v: {}'.format(dist_w2v))\n","## dist_w2v: 0.11"],"execution_count":18,"outputs":[{"output_type":"error","ename":"UnicodeDecodeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-6b7f800ea631>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_vectors_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPRETRAINED_VECTORS_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mvectors_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdist_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors_w2v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectors_w2v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sent2vec/vectorizer.py\u001b[0m in \u001b[0;36mword2vec\u001b[0;34m(self, words, pretrained_vectors_path, ensemble_method)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_vectors_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensemble_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'average'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_vectors_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unexpected end of input; is count incorrect or file otherwise damaged?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mline_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36many2unicode\u001b[0;34m(text, encoding, errors)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x94 in position 7: invalid start byte"]}]}]}