{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"0-basicNLPtasks.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO0ZqkmMqiRmS/G1KUuOnZD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WfWvRNjOSAnL"},"source":["# Preprocessing tasks\n","\n","In this notebook, we will study some of the basic NLP tasks that  that serve to develop more ambitious NLP applications.\n","\n","In particular, we will learn about:\n","\n","\n","1.   Basic tasks such as tokenization, sentence spliting, PoS tagging, lemmatization and stemming.\n","3.   Stopwords\n","4.   Vectorization (from words to vectors)\n","        * Bag of words (BoW) model\n","        * Tf-idf model\n","        \n","\n"]},{"cell_type":"markdown","metadata":{"id":"XeO7t0ZTTY0Z"},"source":["## 1. Basic tasks: tokenization, sentence splitting, PoS taggin, lematization, and stemming.\n","\n","* sentence splitting: the taks of splitting an input text into sentences.\n","* tokenization: the task of segmenting an input text into words (tokens).\n","* PoS tagging: consists of assigning to each word its PoS tag. \n","* lemmatization: given a word, returns its lemma\n","* stemming: given a word, returns its root\n","\n","\n","There are several NLP libraries  that already performe these tasks for us. We will use Spacy in this notebook"]},{"cell_type":"code","metadata":{"id":"wJoyNjCyR9BQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625231793029,"user_tz":-120,"elapsed":6857,"user":{"displayName":"ISABEL SEGURA BEDMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQ1FNrqSnUE2MEN5-mH-AjUf5Ch9orkmSxxfel=s64","userId":"10362143810849156637"}},"outputId":"125e629b-f191-4915-d5e6-9c492fb1f509"},"source":["!python3 -m spacy download en_core_web_sm\n","\n","import spacy\n","\n","nlp = spacy.load('en_core_web_sm')           # load model package \"en_core_web_sm\"\n","\n","print('spacy.en loaded')\n","\n","\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n","Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.0.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.5.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n","spacy.en loaded\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"heqSL8V2Uriw","executionInfo":{"status":"ok","timestamp":1625232011599,"user_tz":-120,"elapsed":353,"user":{"displayName":"ISABEL SEGURA BEDMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQ1FNrqSnUE2MEN5-mH-AjUf5Ch9orkmSxxfel=s64","userId":"10362143810849156637"}},"outputId":"06ba699c-4ef2-4e94-c7f4-36255f6804c7"},"source":["text=\"\"\"Dozens of people have died in Canada amid an unprecedented heatwave that has smashed temperature records. Police in the Vancouver area have responded to more than 130 sudden deaths since Friday. Most were elderly or had underlying health conditions, with heat often a contributing factor. Canada broke its temperature record for a third straight day on Tuesday - 49.6C (121.3F) in Lytton, British Columbia.The US north-west has also seen record highs - and a number of fatalities. Experts say climate change is expected to increase the frequency of extreme weather events, such as heatwaves. However, linking any single event to global warming is complicated. US President Joe Biden said the heatwave was tied to climate change in a speech on Tuesday as he pitched a plan to update the country's infrastructure network. On Wednesday he is meeting with governors of western US states and fire officials, as the annual North American wildfire season begins. The heat over western parts of Canada and the US has been caused by a dome of static high-pressure hot air stretching from California to the Arctic territories. Temperatures have been easing in coastal areas but there is not much respite for inland regions. Before Sunday, temperatures in Canada had never passed 45C.\"\"\"\n","\n","document = nlp(text)\n","\n","print(\"Sentences: \")\n","for i,s in enumerate(document.sents):\n","    print(i,s)\n","    #for token in s:\n","    #  print('\\t',token.orth_, token.pos_)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Sentences: \n","0 Dozens of people have died in Canada amid an unprecedented heatwave that has smashed temperature records.\n","1 Police in the Vancouver area have responded to more than 130 sudden deaths since Friday.\n","2 Most were elderly or had underlying health conditions, with heat often a contributing factor.\n","3 Canada broke its temperature record for a third straight day on Tuesday - 49.6C (121.3F) in Lytton, British Columbia.\n","4 The US north-west has also seen record highs - and a number of fatalities.\n","5 Experts say climate change is expected to increase the frequency of extreme weather events, such as heatwaves.\n","6 However, linking any single event to global warming is complicated.\n","7 US President Joe Biden said the heatwave was tied to climate change in a speech on Tuesday as he pitched a plan to update the country's infrastructure network.\n","8 On Wednesday he is meeting with governors of western US states and fire officials, as the annual North American wildfire season begins.\n","9 The heat over western parts of Canada and the US has been caused by a dome of static high-pressure hot air stretching from California to the Arctic territories.\n","10 Temperatures have been easing in coastal areas but there is not much respite for inland regions.\n","11 Before Sunday, temperatures in Canada had never passed 45C.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7PecNew5WD21"},"source":["# Tokenization and Pos tagging"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hbQig6xsV-4I","executionInfo":{"status":"ok","timestamp":1625232017428,"user_tz":-120,"elapsed":357,"user":{"displayName":"ISABEL SEGURA BEDMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQ1FNrqSnUE2MEN5-mH-AjUf5Ch9orkmSxxfel=s64","userId":"10362143810849156637"}},"outputId":"6b971031-6562-4f96-95c0-cf8623758212"},"source":["for i,s in enumerate(document.sents):\n","    print(i,s)\n","    for token in s:\n","      print('\\t',token.orth_, token.pos_)\n","    break"],"execution_count":8,"outputs":[{"output_type":"stream","text":["0 Dozens of people have died in Canada amid an unprecedented heatwave that has smashed temperature records.\n","\t Dozens NOUN\n","\t of ADP\n","\t people NOUN\n","\t have AUX\n","\t died VERB\n","\t in ADP\n","\t Canada PROPN\n","\t amid ADP\n","\t an DET\n","\t unprecedented ADJ\n","\t heatwave NOUN\n","\t that DET\n","\t has AUX\n","\t smashed VERB\n","\t temperature NOUN\n","\t records NOUN\n","\t . PUNCT\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"h5V7yUBKXG5B"},"source":["Spacy also provides other useful features:"]},{"cell_type":"code","metadata":{"id":"hUi48bHgXFVu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625232074784,"user_tz":-120,"elapsed":344,"user":{"displayName":"ISABEL SEGURA BEDMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQ1FNrqSnUE2MEN5-mH-AjUf5Ch9orkmSxxfel=s64","userId":"10362143810849156637"}},"outputId":"b51f2333-708f-4899-9100-c19eb2b127e2"},"source":["for i, token in enumerate(document):\n","    print(\"original:\", token.orth_)\n","    print(\"shape:\", token.shape_)\n","    print(\"PoS tag:\", token.pos_)\n","\n","\n","    #print(\"lowercased:\", token.lower_)\n","    print(\"lemma:\", token.lemma_)\n","    print(\"prefix:\", token.prefix_)\n","    print(\"suffix:\", token.suffix_)\n","    print(\"----------------------------------------\")\n","    #only shows three first tokens\n","    if i > 5:\n","        break\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["original: Dozens\n","shape: Xxxxx\n","PoS tag: NOUN\n","lemma: dozen\n","prefix: D\n","suffix: ens\n","----------------------------------------\n","original: of\n","shape: xx\n","PoS tag: ADP\n","lemma: of\n","prefix: o\n","suffix: of\n","----------------------------------------\n","original: people\n","shape: xxxx\n","PoS tag: NOUN\n","lemma: people\n","prefix: p\n","suffix: ple\n","----------------------------------------\n","original: have\n","shape: xxxx\n","PoS tag: AUX\n","lemma: have\n","prefix: h\n","suffix: ave\n","----------------------------------------\n","original: died\n","shape: xxxx\n","PoS tag: VERB\n","lemma: die\n","prefix: d\n","suffix: ied\n","----------------------------------------\n","original: in\n","shape: xx\n","PoS tag: ADP\n","lemma: in\n","prefix: i\n","suffix: in\n","----------------------------------------\n","original: Canada\n","shape: Xxxxx\n","PoS tag: PROPN\n","lemma: Canada\n","prefix: C\n","suffix: ada\n","----------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6hY-9O2le46q"},"source":["## Lemmatization and stemming\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fZ5AnZ4ze94d","executionInfo":{"status":"ok","timestamp":1625232423253,"user_tz":-120,"elapsed":1600,"user":{"displayName":"ISABEL SEGURA BEDMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQ1FNrqSnUE2MEN5-mH-AjUf5Ch9orkmSxxfel=s64","userId":"10362143810849156637"}},"outputId":"699cc752-a9f9-4cf9-eb6f-013d1cc5cbe9"},"source":["import nltk\n","from nltk.stem.porter import *\n","stemmer = PorterStemmer()\n","\n","tokens = ['studies', 'studied', 'studying', 'student']\n","text=' '.join(tokens)\n","\n","\n","#print(sentence)\n","for word in nlp(text):\n","    print('word: ' + word.text + '\\tlemma:'+ word.lemma_+ \"\\tstem:\"+stemmer.stem(word.text))\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["word: studies\tlemma:study\tstem:studi\n","word: studied\tlemma:study\tstem:studi\n","word: studying\tlemma:study\tstem:studi\n","word: student\tlemma:student\tstem:student\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2JzFJgfQX-V0"},"source":["## Removing stopwords\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xKeAK5lZYAvG","executionInfo":{"status":"ok","timestamp":1625232815200,"user_tz":-120,"elapsed":359,"user":{"displayName":"ISABEL SEGURA BEDMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQ1FNrqSnUE2MEN5-mH-AjUf5Ch9orkmSxxfel=s64","userId":"10362143810849156637"}},"outputId":"07d5f79d-ec62-47bd-eb27-52b4d543900e"},"source":["from spacy.lang.en import English\n","from spacy.lang.en.stop_words import STOP_WORDS\n","\n","print(STOP_WORDS)\n","# Load English tokenizer, tagger, parser, NER and word vectors\n","nlp = English()\n","\n","text='There are 25 children, who were playing, while their parents were chatting.'\n","text=text.lower()\n","\n","my_doc = nlp(text)\n","\n","# Create list of word tokens\n","token_list = []\n","# Create list of word tokens after removing stopwords\n","filtered =[] \n","\n","for token in my_doc:\n","    word=token.text\n","    token_list.append(token.text)\n","    lexeme = nlp.vocab[token.text]\n","    if lexeme.is_stop == False:\n","        filtered.append(token.text)\n","        \n","\n","\n","print(\"Input Sentence: \\t{}\".format(\" \".join(token_list)))\n","s=\" \".join(filtered)\n","print(\"Text without stopwords: \\t{}\".format(s))\n"],"execution_count":14,"outputs":[{"output_type":"stream","text":["{'some', 'same', 'wherever', '’ll', 'only', 'nothing', 'that', 'am', 'around', 'which', 'is', 'sixty', 'been', 'therefore', 'rather', 'third', 'across', 'amongst', 'seems', 'during', 'can', 'may', 'although', 'keep', 'fifteen', 'yourself', 'thereby', 'latter', 'mostly', 'myself', '‘s', \"'re\", 'could', 'hereby', 'on', 'various', 'him', 'whatever', 'call', 'done', 'quite', 'would', 'again', 'nor', 'see', 'whither', 'to', 'with', '’re', 'must', 'below', 'should', 'besides', 'sometimes', 'until', 'get', 'seemed', 'in', 'whereupon', 'those', 'towards', 'whose', 'whom', 'hers', 'whereafter', 'down', 'thereupon', 'first', \"'m\", 'under', \"'ve\", '’m', 'four', 'name', 'however', 'thence', 'it', 'nine', 'just', 'several', 'this', 'not', 'there', 'using', 'five', 'once', 'but', 'yours', 'of', '‘d', '’d', 'meanwhile', 'both', 'least', 'or', 'almost', 'toward', 'though', 'therein', 'most', 'did', 'nevertheless', 'part', 'how', 'much', 'ours', 'who', 'anywhere', 'beside', 'anything', 'each', 'next', 'enough', 'what', 'here', 'made', 'everyone', 'my', 'among', 'noone', 'go', 'being', 'herself', 'latterly', 'no', 'one', \"'ll\", 'their', 'through', 'why', 'because', 'regarding', 'become', 'full', 'ca', 'perhaps', 'will', 'side', 'due', 'together', 'alone', 'so', 'further', 'other', 'namely', 'at', 'per', 'off', 'out', 'whenever', 'already', 'few', 'beyond', 'becomes', 'well', 'somehow', 'every', 'hundred', 'except', 'even', 'afterwards', 'yourselves', 'her', \"n't\", 'either', 'back', 'up', 'himself', 'does', 'an', 'such', 'them', 'sometime', 're', 'for', 'whoever', 'have', 'without', 'else', '’s', 'do', 'whole', 'anyone', 'he', 'moreover', 'thereafter', 'used', 'also', 'still', 'make', '‘re', 'cannot', 'six', 'any', 'elsewhere', 'three', 'was', 'a', 'me', 'herein', 'are', 'before', 'own', 'all', 'where', 'they', 'our', 'now', 'empty', 'really', 'otherwise', 'themselves', 'too', 'somewhere', 'along', 'we', 'if', 'these', '‘m', 'whereby', 'fifty', 'while', 'wherein', 'by', 'front', 'always', 'ourselves', 'something', 'whether', '’ve', 'nowhere', 'might', 'show', 'then', 'had', 'more', 'thus', 'itself', 'your', 'say', 'us', 'two', 'move', 'the', 'anyhow', 'last', 'against', 'often', 'take', 'throughout', 'many', 'seem', 'eight', '‘ve', 'since', 'you', 'everything', 'than', 'upon', 'forty', 'above', 'former', 'over', 'serious', 'never', 'please', 'whence', 'from', 'about', 'nobody', 'seeming', 'his', 'whereas', 'thru', 'ever', 'beforehand', 'twelve', 'twenty', 'mine', 'hence', 'put', 'eleven', 'after', 'give', 'others', 'less', '‘ll', 'be', 'i', 'amount', 'unless', 'none', 'were', 'she', 'became', 'via', 'becoming', 'someone', 'top', 'anyway', 'neither', 'its', 'as', 'has', 'very', 'everywhere', \"'s\", 'between', 'bottom', 'n‘t', 'within', 'hereupon', 'indeed', 'hereafter', 'ten', 'yet', \"'d\", 'formerly', 'into', 'when', 'onto', 'and', 'n’t', 'doing', 'another', 'behind'}\n","Input Sentence: \tthere are 25 children , who were playing , while their parents were chatting .\n","Text without stopwords: \t25 children , playing , parents chatting .\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aw8hN5NvaB18","executionInfo":{"status":"ok","timestamp":1625232517334,"user_tz":-120,"elapsed":374,"user":{"displayName":"ISABEL SEGURA BEDMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQ1FNrqSnUE2MEN5-mH-AjUf5Ch9orkmSxxfel=s64","userId":"10362143810849156637"}},"outputId":"260d670f-3404-4288-da59-41519829c550"},"source":["from spacy.lang.en import English\n","from spacy.lang.en.stop_words import STOP_WORDS\n","\n","print(STOP_WORDS)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["{'some', 'same', 'wherever', '’ll', 'only', 'nothing', 'that', 'am', 'around', 'which', 'is', 'sixty', 'been', 'therefore', 'rather', 'third', 'across', 'amongst', 'seems', 'during', 'can', 'may', 'although', 'keep', 'fifteen', 'yourself', 'thereby', 'latter', 'mostly', 'myself', '‘s', \"'re\", 'could', 'hereby', 'on', 'various', 'him', 'whatever', 'call', 'done', 'quite', 'would', 'again', 'nor', 'see', 'whither', 'to', 'with', '’re', 'must', 'below', 'should', 'besides', 'sometimes', 'until', 'get', 'seemed', 'in', 'whereupon', 'those', 'towards', 'whose', 'whom', 'hers', 'whereafter', 'down', 'thereupon', 'first', \"'m\", 'under', \"'ve\", '’m', 'four', 'name', 'however', 'thence', 'it', 'nine', 'just', 'several', 'this', 'not', 'there', 'using', 'five', 'once', 'but', 'yours', 'of', '‘d', '’d', 'meanwhile', 'both', 'least', 'or', 'almost', 'toward', 'though', 'therein', 'most', 'did', 'nevertheless', 'part', 'how', 'much', 'ours', 'who', 'anywhere', 'beside', 'anything', 'each', 'next', 'enough', 'what', 'here', 'made', 'everyone', 'my', 'among', 'noone', 'go', 'being', 'herself', 'latterly', 'no', 'one', \"'ll\", 'their', 'through', 'why', 'because', 'regarding', 'become', 'full', 'ca', 'perhaps', 'will', 'side', 'due', 'together', 'alone', 'so', 'further', 'other', 'namely', 'at', 'per', 'off', 'out', 'whenever', 'already', 'few', 'beyond', 'becomes', 'well', 'somehow', 'every', 'hundred', 'except', 'even', 'afterwards', 'yourselves', 'her', \"n't\", 'either', 'back', 'up', 'himself', 'does', 'an', 'such', 'them', 'sometime', 're', 'for', 'whoever', 'have', 'without', 'else', '’s', 'do', 'whole', 'anyone', 'he', 'moreover', 'thereafter', 'used', 'also', 'still', 'make', '‘re', 'cannot', 'six', 'any', 'elsewhere', 'three', 'was', 'a', 'me', 'herein', 'are', 'before', 'own', 'all', 'where', 'they', 'our', 'now', 'empty', 'really', 'otherwise', 'themselves', 'too', 'somewhere', 'along', 'we', 'if', 'these', '‘m', 'whereby', 'fifty', 'while', 'wherein', 'by', 'front', 'always', 'ourselves', 'something', 'whether', '’ve', 'nowhere', 'might', 'show', 'then', 'had', 'more', 'thus', 'itself', 'your', 'say', 'us', 'two', 'move', 'the', 'anyhow', 'last', 'against', 'often', 'take', 'throughout', 'many', 'seem', 'eight', '‘ve', 'since', 'you', 'everything', 'than', 'upon', 'forty', 'above', 'former', 'over', 'serious', 'never', 'please', 'whence', 'from', 'about', 'nobody', 'seeming', 'his', 'whereas', 'thru', 'ever', 'beforehand', 'twelve', 'twenty', 'mine', 'hence', 'put', 'eleven', 'after', 'give', 'others', 'less', '‘ll', 'be', 'i', 'amount', 'unless', 'none', 'were', 'she', 'became', 'via', 'becoming', 'someone', 'top', 'anyway', 'neither', 'its', 'as', 'has', 'very', 'everywhere', \"'s\", 'between', 'bottom', 'n‘t', 'within', 'hereupon', 'indeed', 'hereafter', 'ten', 'yet', \"'d\", 'formerly', 'into', 'when', 'onto', 'and', 'n’t', 'doing', 'another', 'behind'}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vjyxPaTYZHQn"},"source":["## Removing puntuaction, special characters and numbers\n","\n","More examples: \n","https://github.com/isegura/BasicNLP/tree/master/RegEx"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mmTD2pbEaL68","executionInfo":{"status":"ok","timestamp":1625232828659,"user_tz":-120,"elapsed":370,"user":{"displayName":"ISABEL SEGURA BEDMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQ1FNrqSnUE2MEN5-mH-AjUf5Ch9orkmSxxfel=s64","userId":"10362143810849156637"}},"outputId":"30732384-ee9b-48b4-ba87-2f58c4296f3f"},"source":["import re\n","\n","print('input:', s)\n","clean = re.sub(r'[^\\w\\s]+','',s)\n","print(\"after removing puntuaction, special characters: \", clean)\n","clean = re.sub(r'[\\d]+','',clean)\n","print(\"after removing numbers: \", clean)\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["input: 25 children , playing , parents chatting .\n","after removing puntuaction, special characters:  25 children  playing  parents chatting \n","after removing numbers:   children  playing  parents chatting \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aOZ1uLckdYeI"},"source":["# More... \n","\n","* Named Entity Recognition with Spacy: https://github.com/isegura/BasicNLP/blob/master/NER/IntroNER_spacy.ipynb\n","* Noun chunker and parsing: https://github.com/isegura/BasicNLP/blob/master/TextProccesing/SpaCy_NLP.ipynb\n","* Word embeddings with Spacy: https://github.com/isegura/BasicNLP/blob/master/TextSimilarity/Word_Embeddings_By_Spacy.ipynb\n","\n"]}]}